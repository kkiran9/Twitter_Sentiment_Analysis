{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aarush\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim-3.0.1-py3.5-win-amd64.egg\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "import keras.layers as layer\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_model(X_test, y_test, model):\n",
    "    scores = model.evaluate(X_test, y_test)\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Pass the dataset with data in the format: class:content\"\"\"\n",
    "\"\"\"Text pre-processing: Removing links, special characters, and digits. df_column[1] is also converted into lower case\"\"\"\n",
    "def preprocess_dataset(df, df_column):\n",
    "    df[df_column[1]] = df[df_column[1]].str.replace('-', ' ')\n",
    "    df[df_column[1]] = df[df_column[1]].str.replace('(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '')\n",
    "    df[df_column[1]] = df[df_column[1]].str.replace('[^a-zA-Z0-9 \\n]', ' ')\n",
    "    df[df_column[1]] = df[df_column[1]].str.replace('\\d+', '')\n",
    "    df[df_column[1]] = df[df_column[1]].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"To label the sentiment classes using integers. Not to be used for the neural network\"\"\"\n",
    "def to_categorical(df):\n",
    "    df.sentiment = pd.Categorical(df.sentiment)\n",
    "    df['class'] = df.sentiment.cat.codes\n",
    "    return df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Function returns the one-hot representation of the sentiment classes\"\"\"\n",
    "def to_OneHot(df, df_columns):\n",
    "    b = pd.get_dummies(df[df_column[0]], prefix=\"\")\n",
    "    list1 = list(b)\n",
    "    OneHot = b[list1[0]]\n",
    "    OneHot = np.column_stack(b[list1[i]] for i in range(len(list1)))\n",
    "    print(len(list1))\n",
    "    print(OneHot)\n",
    "    return OneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache  ughhhh   waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony   gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dannycastillo we want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty   tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  layin n bed with a headache  ughhhh   waitin o...\n",
       "2     sadness                funeral ceremony   gloomy friday   \n",
       "3  enthusiasm               wants to hang out with friends soon \n",
       "4     neutral   dannycastillo we want to trade with someone w..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Datasets/Twitter Sentiment Analysis/train_data.csv')\n",
    "df_column = list(df)      #Names of the columns of the dataframe\n",
    "classes = df[df_column[0]].unique().size #Number of distinct classes for the dataset. 13 for the given dataset\n",
    "df = preprocess_dataset(df, df_column)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "[[0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 1 0 0]\n",
      " [0 0 0 ..., 1 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "labels = to_OneHot(df, df_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[df_column[1]], labels, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.to_frame().reset_index()\n",
    "X_test = X_test.to_frame()\n",
    "# print(y_train.shape)\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sent_list = [list(filter(None, row['content'].split(\" \"))) for i, row in X_train.iterrows()]\n",
    "test_sent_list = [list(filter(None, row['content'].split(\" \"))) for i, row in X_test.iterrows()]\n",
    "# print(text_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text_list_reduced = []\n",
    "# for i in range(len(train_sent_list)):\n",
    "#     text_list_reduced.append([word for word in train_sent_list[i] if word not in stopwords.words('english')])\n",
    "# train_sent_list = text_list_reduced\n",
    "# del text_list_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('wishes' not in train_sent_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Vocabulary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for i in range(len(train_sent_list)):\n",
    "    for j in range(len(train_sent_list[i])):    \n",
    "        if train_sent_list[i][j] not in vocab:\n",
    "            vocab.append(train_sent_list[i][j])\n",
    "#     if len(text_list[i]) != 0:\n",
    "#         np.asarray(X[i]/len(text_list[i]))\n",
    "# X = np.asarray(X)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31721"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting sentence list to Network input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Making sentence list using vocab indices\"\"\"\n",
    "X_train_indexed  = []\n",
    "for i in range(len(train_sent_list)):\n",
    "    X_train_indexed.append([])\n",
    "    for j in range(len(train_sent_list[i])):\n",
    "        X_train_indexed[i].append(vocab.index(train_sent_list[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_indexed = []\n",
    "for i in range(len(test_sent_list)):\n",
    "    X_test_indexed.append([])\n",
    "    for j in range(len(test_sent_list[i])):\n",
    "        if(test_sent_list[i][j] in vocab):\n",
    "            X_test_indexed[i].append(vocab.index(test_sent_list[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_reduced = keras.preprocessing.sequence.pad_sequences(X_train_indexed)\n",
    "max_sent_length = len(X_train_reduced[0])\n",
    "X_test_reduced= keras.preprocessing.sequence.pad_sequences(X_test_indexed, maxlen=max_sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_reduced[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelDef(vocab_size, max_sent_len, embedding_size=128,optimizer = 'adam', loss = 'categorical_crossentropy', plot = False):\n",
    "    model = Sequential()\n",
    "    model.add(layer.embeddings.Embedding(vocab_size, embedding_size, input_length = max_sent_len)) # max vocab index, embedding size, input list size\n",
    "    model.add(layer.convolutional.Conv1D(256, 32, padding = 'same', activation = 'elu'))\n",
    "#     model.add(layer.Dropout(0.2))\n",
    "#     model.add(layer.pooling.MaxPooling1D(pool_size = 8))\n",
    "    model.add(LSTM(128))\n",
    "#     model.add(layer.Flatten())\n",
    "#     model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(13, activation = 'softmax'))\n",
    "    model.compile(optimizer = optimizer, loss=loss, metrics = ['accuracy'] )\n",
    "    if(plot):\n",
    "        plot_model(model, to_file = 'model.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = modelDef(vocab_size=len(vocab), max_sent_len=len(X_train_reduced[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aarush\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras-2.0.8-py3.5.egg\\keras\\models.py:852: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_reduced, y_train, batch_size = 1080, nb_epoch = 3, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_model(X_test_reduced, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
